/**
 * ğŸ¤– Hybrid LLM System for Raqim AI 966
 * 
 * Ù†Ø¸Ø§Ù… Ø°ÙƒÙŠ ÙŠØ¯Ø¹Ù… Ø¹Ø¯Ø© Ù†Ù…Ø§Ø°Ø¬ LLM Ù…Ø¹ ØªÙˆØ¬ÙŠÙ‡ ØªÙ„Ù‚Ø§Ø¦ÙŠ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰:
 * - Ù†ÙˆØ¹ Ø§Ù„Ù…Ù‡Ù…Ø© (ØªÙˆÙ„ÙŠØ¯ØŒ ØªØ­Ù„ÙŠÙ„ØŒ ØªØ±Ø¬Ù…Ø©)
 * - Ø­Ø¬Ù… Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„
 * - Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ© (Ø³Ø±Ø¹Ø©ØŒ Ø¬ÙˆØ¯Ø©ØŒ ØªÙƒÙ„ÙØ©)
 * - ØªÙˆÙØ± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
 */

import axios, { AxiosInstance } from 'axios';

// ===== ØªØ¹Ø±ÙŠÙ Ø§Ù„Ø£Ù†ÙˆØ§Ø¹ =====

export type LLMProvider = 'gemini' | 'deepseek' | 'claude' | 'gpt4' | 'humain';

export type TaskType = 
  | 'generate_prompt'      // ØªÙˆÙ„ÙŠØ¯ Ø¨Ø±ÙˆÙ…Ø¨Øª
  | 'analyze_prompt'       // ØªØ­Ù„ÙŠÙ„ Ø¨Ø±ÙˆÙ…Ø¨Øª
  | 'translate'            // ØªØ±Ø¬Ù…Ø©
  | 'summarize'            // ØªÙ„Ø®ÙŠØµ
  | 'creative_writing'     // ÙƒØªØ§Ø¨Ø© Ø¥Ø¨Ø¯Ø§Ø¹ÙŠØ©
  | 'code_generation'      // ØªÙˆÙ„ÙŠØ¯ ÙƒÙˆØ¯
  | 'general';             // Ø¹Ø§Ù…

export type Priority = 'speed' | 'quality' | 'cost' | 'balanced';

export interface LLMConfig {
  provider: LLMProvider;
  apiUrl: string;
  apiKey: string;
  model: string;
  maxTokens: number;
  temperature: number;
  enabled: boolean;
  costPer1kTokens: number;  // Ø¨Ø§Ù„Ø¯ÙˆÙ„Ø§Ø±
  avgResponseTime: number;   // Ø¨Ø§Ù„Ø«ÙˆØ§Ù†ÙŠ
  qualityScore: number;      // Ù…Ù† 1-10
}

export interface LLMRequest {
  prompt: string;
  taskType?: TaskType;
  priority?: Priority;
  maxTokens?: number;
  temperature?: number;
  preferredProvider?: LLMProvider;
}

export interface LLMResponse {
  content: string;
  provider: LLMProvider;
  model: string;
  tokensUsed: number;
  cost: number;
  responseTime: number;
  success: boolean;
  error?: string;
}

// ===== ØªÙƒÙˆÙŠÙ† Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ =====

export class HybridLLMSystem {
  private configs: Map<LLMProvider, LLMConfig> = new Map();
  private clients: Map<LLMProvider, AxiosInstance> = new Map();
  private usageStats: Map<LLMProvider, { requests: number; tokens: number; cost: number }> = new Map();

  constructor() {
    this.initializeConfigs();
    this.initializeClients();
    this.initializeStats();
  }

  /**
   * ØªÙ‡ÙŠØ¦Ø© ØªÙƒÙˆÙŠÙ†Ø§Øª Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
   */
  private initializeConfigs(): void {
    // Google Gemini
    this.configs.set('gemini', {
      provider: 'gemini',
      apiUrl: process.env.LLM_API_URL || 'https://generativelanguage.googleapis.com',
      apiKey: process.env.LLM_API_KEY || '',
      model: process.env.GEMINI_MODEL || 'gemini-1.5-flash',
      maxTokens: 8192,
      temperature: 0.7,
      enabled: !!process.env.LLM_API_KEY,
      costPer1kTokens: 0.00025,
      avgResponseTime: 2.5,
      qualityScore: 8
    });

    // DeepSeek
    this.configs.set('deepseek', {
      provider: 'deepseek',
      apiUrl: process.env.DEEPSEEK_API_URL || 'https://api.deepseek.com',
      apiKey: process.env.DEEPSEEK_API_KEY || '',
      model: process.env.DEEPSEEK_MODEL || 'deepseek-chat',
      maxTokens: 8192,
      temperature: 0.7,
      enabled: !!process.env.DEEPSEEK_API_KEY,
      costPer1kTokens: 0.00014,
      avgResponseTime: 3.0,
      qualityScore: 7
    });

    // Claude (Anthropic)
    this.configs.set('claude', {
      provider: 'claude',
      apiUrl: process.env.CLAUDE_API_URL || 'https://api.anthropic.com',
      apiKey: process.env.CLAUDE_API_KEY || '',
      model: process.env.CLAUDE_MODEL || 'claude-3-haiku-20240307',
      maxTokens: 4096,
      temperature: 0.7,
      enabled: !!process.env.CLAUDE_API_KEY,
      costPer1kTokens: 0.00025,
      avgResponseTime: 2.0,
      qualityScore: 9
    });

    // GPT-4 (OpenAI)
    this.configs.set('gpt4', {
      provider: 'gpt4',
      apiUrl: process.env.OPENAI_API_URL || 'https://api.openai.com',
      apiKey: process.env.OPENAI_API_KEY || '',
      model: process.env.OPENAI_MODEL || 'gpt-4o-mini',
      maxTokens: 4096,
      temperature: 0.7,
      enabled: !!process.env.OPENAI_API_KEY,
      costPer1kTokens: 0.00015,
      avgResponseTime: 3.5,
      qualityScore: 9
    });

    // HUMAIN (Local Saudi Model)
    this.configs.set('humain', {
      provider: 'humain',
      apiUrl: process.env.HUMAIN_API_URL || 'https://api.humain.sa',
      apiKey: process.env.HUMAIN_API_KEY || '',
      model: process.env.HUMAIN_MODEL || 'humain-1',
      maxTokens: 4096,
      temperature: 0.7,
      enabled: !!process.env.HUMAIN_API_KEY,
      costPer1kTokens: 0.0001,
      avgResponseTime: 2.5,
      qualityScore: 8
    });
  }

  /**
   * ØªÙ‡ÙŠØ¦Ø© Ø¹Ù…Ù„Ø§Ø¡ HTTP Ù„ÙƒÙ„ Ù†Ù…ÙˆØ°Ø¬
   */
  private initializeClients(): void {
    this.configs.forEach((config, provider) => {
      if (config.enabled) {
        this.clients.set(provider, axios.create({
          baseURL: config.apiUrl,
          timeout: 30000,
          headers: this.getHeaders(provider, config)
        }));
      }
    });
  }

  /**
   * ØªÙ‡ÙŠØ¦Ø© Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
   */
  private initializeStats(): void {
    this.configs.forEach((_, provider) => {
      this.usageStats.set(provider, {
        requests: 0,
        tokens: 0,
        cost: 0
      });
    });
  }

  /**
   * Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Headers Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø© Ù„ÙƒÙ„ Ù†Ù…ÙˆØ°Ø¬
   */
  private getHeaders(provider: LLMProvider, config: LLMConfig): Record<string, string> {
    const headers: Record<string, string> = {
      'Content-Type': 'application/json'
    };

    switch (provider) {
      case 'gemini':
        headers['x-goog-api-key'] = config.apiKey;
        break;
      case 'claude':
        headers['x-api-key'] = config.apiKey;
        headers['anthropic-version'] = '2023-06-01';
        break;
      case 'gpt4':
        headers['Authorization'] = `Bearer ${config.apiKey}`;
        break;
      case 'deepseek':
      case 'humain':
        headers['Authorization'] = `Bearer ${config.apiKey}`;
        break;
    }

    return headers;
  }

  /**
   * ğŸ¯ Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£Ù…Ø«Ù„ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù‡Ù…Ø© ÙˆØ§Ù„Ø£ÙˆÙ„ÙˆÙŠØ©
   */
  selectBestProvider(taskType: TaskType, priority: Priority): LLMProvider {
    const availableProviders = Array.from(this.configs.entries())
      .filter(([_, config]) => config.enabled)
      .map(([provider, config]) => ({ provider, config }));

    if (availableProviders.length === 0) {
      throw new Error('âŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†Ù…Ø§Ø°Ø¬ LLM Ù…ØªØ§Ø­Ø©!');
    }

    // Ø­Ø³Ø¨ Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ©
    switch (priority) {
      case 'speed':
        return this.selectBySpeed(availableProviders);
      case 'quality':
        return this.selectByQuality(availableProviders, taskType);
      case 'cost':
        return this.selectByCost(availableProviders);
      case 'balanced':
      default:
        return this.selectBalanced(availableProviders, taskType);
    }
  }

  /**
   * Ø§Ø®ØªÙŠØ§Ø± Ø­Ø³Ø¨ Ø§Ù„Ø³Ø±Ø¹Ø©
   */
  private selectBySpeed(providers: Array<{ provider: LLMProvider; config: LLMConfig }>): LLMProvider {
    return providers.reduce((fastest, current) => 
      current.config.avgResponseTime < fastest.config.avgResponseTime ? current : fastest
    ).provider;
  }

  /**
   * Ø§Ø®ØªÙŠØ§Ø± Ø­Ø³Ø¨ Ø§Ù„Ø¬ÙˆØ¯Ø©
   */
  private selectByQuality(providers: Array<{ provider: LLMProvider; config: LLMConfig }>, taskType: TaskType): LLMProvider {
    // Ù†Ù…Ø§Ø°Ø¬ Ù…Ø¹ÙŠÙ†Ø© Ø£ÙØ¶Ù„ Ù„Ù…Ù‡Ø§Ù… Ù…Ø¹ÙŠÙ†Ø©
    const taskPreferences: Record<TaskType, LLMProvider[]> = {
      generate_prompt: ['deepseek', 'claude', 'gpt4'],
      analyze_prompt: ['deepseek', 'claude', 'gpt4'],
      translate: ['deepseek', 'gpt4', 'humain'],
      summarize: ['deepseek', 'claude', 'gpt4'],
      creative_writing: ['gpt4', 'deepseek', 'claude'],
      code_generation: ['deepseek', 'gpt4', 'claude'],
      general: ['deepseek', 'claude', 'gpt4']
    };

    const preferred = taskPreferences[taskType] || taskPreferences.general;
    
    for (const provider of preferred) {
      if (providers.some(p => p.provider === provider)) {
        return provider;
      }
    }

    // Ø¥Ø°Ø§ Ù„Ù… ÙŠØªÙˆÙØ± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…ÙØ¶Ù„ØŒ Ø§Ø®ØªØ± Ø§Ù„Ø£Ø¹Ù„Ù‰ Ø¬ÙˆØ¯Ø©
    return providers.reduce((best, current) => 
      current.config.qualityScore > best.config.qualityScore ? current : best
    ).provider;
  }

  /**
   * Ø§Ø®ØªÙŠØ§Ø± Ø­Ø³Ø¨ Ø§Ù„ØªÙƒÙ„ÙØ©
   */
  private selectByCost(providers: Array<{ provider: LLMProvider; config: LLMConfig }>): LLMProvider {
    return providers.reduce((cheapest, current) => 
      current.config.costPer1kTokens < cheapest.config.costPer1kTokens ? current : cheapest
    ).provider;
  }

  /**
   * Ø§Ø®ØªÙŠØ§Ø± Ù…ØªÙˆØ§Ø²Ù†
   */
  private selectBalanced(providers: Array<{ provider: LLMProvider; config: LLMConfig }>, taskType: TaskType): LLMProvider {
    // Ù…Ø¹Ø§Ø¯Ù„Ø© Ù…ÙˆØ²ÙˆÙ†Ø©: Ø§Ù„Ø¬ÙˆØ¯Ø© 40% + Ø§Ù„Ø³Ø±Ø¹Ø© 30% + Ø§Ù„ØªÙƒÙ„ÙØ© 30%
    const scored = providers.map(({ provider, config }) => {
      const qualityScore = (config.qualityScore / 10) * 0.4;
      const speedScore = (1 - (config.avgResponseTime / 5)) * 0.3;
      const costScore = (1 - (config.costPer1kTokens / 0.001)) * 0.3;
      const totalScore = qualityScore + speedScore + costScore;

      return { provider, score: totalScore };
    });

    return scored.reduce((best, current) => 
      current.score > best.score ? current : best
    ).provider;
  }

  /**
   * ğŸš€ Ø¥Ø±Ø³Ø§Ù„ Ø·Ù„Ø¨ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬
   */
  async sendRequest(request: LLMRequest): Promise<LLMResponse> {
    const startTime = Date.now();
    const {
      prompt,
      taskType = 'general',
      priority = 'balanced',
      maxTokens,
      temperature,
      preferredProvider
    } = request;

    // Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    const provider = preferredProvider && this.configs.get(preferredProvider)?.enabled
      ? preferredProvider
      : this.selectBestProvider(taskType, priority);

    const config = this.configs.get(provider)!;
    const client = this.clients.get(provider)!;

    try {
      // Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø·Ù„Ø¨ Ø­Ø³Ø¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
      const requestBody = this.prepareRequest(provider, config, {
        prompt,
        maxTokens: maxTokens || config.maxTokens,
        temperature: temperature || config.temperature
      });

      // Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø·Ù„Ø¨
      const response = await client.post(this.getEndpoint(provider), requestBody);

      // Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†ØªÙŠØ¬Ø©
      const content = this.extractContent(provider, response.data);
      const tokensUsed = this.extractTokens(provider, response.data);
      const cost = (tokensUsed / 1000) * config.costPer1kTokens;
      const responseTime = (Date.now() - startTime) / 1000;

      // ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
      this.updateStats(provider, tokensUsed, cost);

      return {
        content,
        provider,
        model: config.model,
        tokensUsed,
        cost,
        responseTime,
        success: true
      };

    } catch (error: any) {
      console.error(`âŒ Ø®Ø·Ø£ ÙÙŠ ${provider}:`, error.message);

      // Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø¹ Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯ÙŠÙ„
      if (!preferredProvider) {
        const fallbackProvider = this.getFallbackProvider(provider);
        if (fallbackProvider) {
          console.log(`ğŸ”„ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø¹ ${fallbackProvider}...`);
          return this.sendRequest({
            ...request,
            preferredProvider: fallbackProvider
          });
        }
      }

      return {
        content: '',
        provider,
        model: config.model,
        tokensUsed: 0,
        cost: 0,
        responseTime: (Date.now() - startTime) / 1000,
        success: false,
        error: error.message
      };
    }
  }

  /**
   * Ø¥Ø¹Ø¯Ø§Ø¯ Ø¬Ø³Ù… Ø§Ù„Ø·Ù„Ø¨ Ø­Ø³Ø¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
   */
  private prepareRequest(provider: LLMProvider, config: LLMConfig, params: any): any {
    const { prompt, maxTokens, temperature } = params;

    switch (provider) {
      case 'gemini':
        return {
          contents: [{
            parts: [{ text: prompt }]
          }],
          generationConfig: {
            maxOutputTokens: maxTokens,
            temperature
          }
        };

      case 'claude':
        return {
          model: config.model,
          max_tokens: maxTokens,
          temperature,
          messages: [{
            role: 'user',
            content: prompt
          }]
        };

      case 'gpt4':
      case 'deepseek':
      case 'humain':
        return {
          model: config.model,
          max_tokens: maxTokens,
          temperature,
          messages: [{
            role: 'user',
            content: prompt
          }]
        };
    }
  }

  /**
   * Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ endpoint Ø§Ù„Ù…Ù†Ø§Ø³Ø¨
   */
  private getEndpoint(provider: LLMProvider): string {
    const config = this.configs.get(provider)!;
    
    switch (provider) {
      case 'gemini':
        return `/v1beta/models/${config.model}:generateContent`;
      case 'claude':
        return '/v1/messages';
      case 'gpt4':
        return '/v1/chat/completions';
      case 'deepseek':
      case 'humain':
        return '/v1/chat/completions';
      default:
        return '/v1/chat/completions';
    }
  }

  /**
   * Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù…Ù† Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©
   */
  private extractContent(provider: LLMProvider, data: any): string {
    switch (provider) {
      case 'gemini':
        return data.candidates?.[0]?.content?.parts?.[0]?.text || '';
      case 'claude':
        return data.content?.[0]?.text || '';
      case 'gpt4':
      case 'deepseek':
      case 'humain':
        return data.choices?.[0]?.message?.content || '';
      default:
        return '';
    }
  }

  /**
   * Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¹Ø¯Ø¯ Ø§Ù„Ù€ tokens
   */
  private extractTokens(provider: LLMProvider, data: any): number {
    switch (provider) {
      case 'gemini':
        return data.usageMetadata?.totalTokenCount || 0;
      case 'claude':
        return (data.usage?.input_tokens || 0) + (data.usage?.output_tokens || 0);
      case 'gpt4':
      case 'deepseek':
      case 'humain':
        return data.usage?.total_tokens || 0;
      default:
        return 0;
    }
  }

  /**
   * Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯ÙŠÙ„
   */
  private getFallbackProvider(failedProvider: LLMProvider): LLMProvider | null {
    const fallbackOrder: LLMProvider[] = ['deepseek', 'claude', 'gpt4', 'gemini', 'humain'];
    const available = fallbackOrder.filter(p => 
      p !== failedProvider && this.configs.get(p)?.enabled
    );
    return available[0] || null;
  }

  /**
   * ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
   */
  private updateStats(provider: LLMProvider, tokens: number, cost: number): void {
    const stats = this.usageStats.get(provider);
    if (stats) {
      stats.requests++;
      stats.tokens += tokens;
      stats.cost += cost;
    }
  }

  /**
   * ğŸ“Š Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
   */
  getUsageStats(): Record<LLMProvider, { requests: number; tokens: number; cost: number }> {
    const stats: any = {};
    this.usageStats.forEach((value, key) => {
      stats[key] = { ...value };
    });
    return stats;
  }

  /**
   * ğŸ“Š Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ØªØ§Ø­Ø©
   */
  getAvailableProviders(): Array<{ provider: LLMProvider; config: Omit<LLMConfig, 'apiKey'> }> {
    return Array.from(this.configs.entries())
      .filter(([_, config]) => config.enabled)
      .map(([provider, config]) => ({
        provider,
        config: {
          ...config,
          apiKey: '***' // Ø¥Ø®ÙØ§Ø¡ API Key
        }
      }));
  }
}

// ===== ØªØµØ¯ÙŠØ± Instance ÙˆØ§Ø­Ø¯ =====
export const hybridLLM = new HybridLLMSystem();
